
#ifndef ORIGINAL_H
#define ORIGINAL_H
// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 1.11.0
// ONNX IR version: 9
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

static const float tensor_Gemm_0_weight[4][4] = 
{
  {0.47023767232894897461f, 0.10226713865995407104f, 0.30230417847633361816f, 1.0616005659103393555f},
  {0.33644628524780273438f, 0.016413867473602294922f, -0.36280584335327148438f, -0.49338215589523315430f},
  {0.77833706140518188477f, -0.59048420190811157227f, 0.84037828445434570312f, 2.0529561042785644531f},
  {0.16459813714027404785f, 0.68735158443450927734f, -0.27445447444915771484f, -1.0277401208877563477f}
};
static const float tensor_Gemm_0_bias[4] = 
{-0.35179343819618225098f, -0.086358606815338134766f, -0.19665037095546722412f, 1.4326065778732299805f};
static const float tensor_Gemm_1_weight[4][4] = 
{
  {-0.0016429345123469829559f, -0.19008547067642211914f, -0.33713096380233764648f, 0.55138891935348510742f},
  {-0.56929159164428710938f, -0.076551020145416259766f, -1.1808722019195556641f, 1.6828993558883666992f},
  {0.88139915466308593750f, 0.39555501937866210938f, 1.9554126262664794922f, -0.41937947273254394531f},
  {-0.35455858707427978516f, -0.44971632957458496094f, -0.29892426729202270508f, -0.27380108833312988281f}
};
static const float tensor_Gemm_1_bias[4] = 
{-0.062056012451648712158f, 2.2832393646240234375f, 0.59419274330139160156f, -0.15332841873168945312f};
static const float tensor_Gemm_2_weight[3][4] = 
{
  {0.42922529578208923340f, 1.9064224958419799805f, -1.7469170093536376953f, 0.49250864982604980469f},
  {-0.15587426722049713135f, 0.95745402574539184570f, 0.47757169604301452637f, 0.43050777912139892578f},
  {-0.079822406172752380371f, -2.2888047695159912109f, 1.4965174198150634766f, -0.24682003259658813477f}
};
static const float tensor_Gemm_2_bias[3] = 
{-0.015897920355200767517f, 0.48216184973716735840f, -0.52612411975860595703f};
float tensor_onnx__Gemm_7[1][4];
float tensor_onnx__Gemm_9[1][4];
float tensor_onnx__Gemm_11[1][4];

float tensor_input[1][4];
float tensor_input_3[1][4];


static inline void node_Flatten_0( const float tensor_onnx__Flatten_0[1][4], float tensor_onnx__Gemm_7[1][4] )
{
	/* Flatten*/
	float *input = (float*)tensor_onnx__Flatten_0;
	float *output = (float*)tensor_onnx__Gemm_7;
	for( uint32_t i=0; i<4; i++ )
		output[i] = input[i];

}

static inline void node_Gemm_1( const float tensor_onnx__Gemm_7[1][4], const float tensor_Gemm_0_weight[4][4], const float tensor_Gemm_0_bias[4], float tensor_input[1][4] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 4;
	float (*A)[4]  = (float(*)[4])tensor_onnx__Gemm_7;
	float (*Y)[4]  = (float(*)[4])tensor_input;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[4]  = (float(*)[4])tensor_Gemm_0_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_0_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node_Relu_2( const float tensor_input[1][4], float tensor_onnx__Gemm_9[1][4] )
{
	/*Relu*/
	float *X = (float*)tensor_input;
	float *Y = (float*)tensor_onnx__Gemm_9;
	for( uint32_t i=0; i<4; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node_Gemm_3( const float tensor_onnx__Gemm_9[1][4], const float tensor_Gemm_1_weight[4][4], const float tensor_Gemm_1_bias[4], float tensor_input_3[1][4] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 4;
	float (*A)[4]  = (float(*)[4])tensor_onnx__Gemm_9;
	float (*Y)[4]  = (float(*)[4])tensor_input_3;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[4]  = (float(*)[4])tensor_Gemm_1_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_1_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node_Relu_4( const float tensor_input_3[1][4], float tensor_onnx__Gemm_11[1][4] )
{
	/*Relu*/
	float *X = (float*)tensor_input_3;
	float *Y = (float*)tensor_onnx__Gemm_11;
	for( uint32_t i=0; i<4; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node_Gemm_5( const float tensor_onnx__Gemm_11[1][4], const float tensor_Gemm_2_weight[3][4], const float tensor_Gemm_2_bias[3], float tensor_12[1][3] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 3;
	float (*A)[4]  = (float(*)[4])tensor_onnx__Gemm_11;
	float (*Y)[3]  = (float(*)[3])tensor_12;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[3]  = (float(*)[3])tensor_Gemm_2_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_2_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}


void original(const float tensor_onnx__Flatten_0[1][4], float tensor_12[1][3]) {
	node_Flatten_0( tensor_onnx__Flatten_0, tensor_onnx__Gemm_7);
	node_Gemm_1( tensor_onnx__Gemm_7, tensor_Gemm_0_weight, tensor_Gemm_0_bias, tensor_input);
	node_Relu_2( tensor_input, tensor_onnx__Gemm_9);
	node_Gemm_3( tensor_onnx__Gemm_9, tensor_Gemm_1_weight, tensor_Gemm_1_bias, tensor_input_3);
	node_Relu_4( tensor_input_3, tensor_onnx__Gemm_11);
	node_Gemm_5( tensor_onnx__Gemm_11, tensor_Gemm_2_weight, tensor_Gemm_2_bias, tensor_12);
}

#endif // ORIGINAL_H

