
#ifndef ORIGINAL_H
#define ORIGINAL_H
// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 1.11.0
// ONNX IR version: 9
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

static const float tensor_Gemm_0_weight[10][7] = 
{
  {1.7546970844268798828f, 0.71623200178146362305f, -0.46786302328109741211f, 0.54403674602508544922f, 1.1822630167007446289f, -1.0489791631698608398f, -0.20417766273021697998f},
  {-1.1450501680374145508f, 0.069851033389568328857f, 0.14349767565727233887f, -0.32986971735954284668f, -0.13236927986145019531f, 1.0783730745315551758f, 0.30718430876731872559f},
  {0.11976234614849090576f, -0.25272893905639648438f, -0.31019598245620727539f, -0.065280966460704803467f, 0.13848784565925598145f, 0.30185142159461975098f, 0.074104860424995422363f},
  {-1.6013461351394653320f, -0.18439073860645294189f, 1.6706635951995849609f, -0.11522823572158813477f, -0.28908327221870422363f, -0.79248321056365966797f, -0.76845765113830566406f},
  {-0.0057959798723459243774f, 0.28243148326873779297f, -0.028734285384416580200f, -0.036241021007299423218f, -0.33361792564392089844f, -0.033427510410547256470f, -0.14097520709037780762f},
  {-1.9109301567077636719f, -0.42343065142631530762f, 0.90871423482894897461f, 0.24424056708812713623f, -0.91706204414367675781f, 1.1050641536712646484f, 0.38778969645500183105f},
  {0.66654330492019653320f, 0.50659942626953125000f, -0.48141646385192871094f, 0.44933536648750305176f, -0.089258208870887756348f, 0.79959094524383544922f, 0.48790970444679260254f},
  {-0.30354565382003784180f, 0.36188638210296630859f, -0.18657840788364410400f, 0.37595221400260925293f, -0.24391368031501770020f, -0.17728966474533081055f, -0.32203575968742370605f},
  {-0.049946311861276626587f, 0.25000649690628051758f, -0.31036722660064697266f, -0.23993831872940063477f, -0.16136772930622100830f, -0.21845194697380065918f, 0.067955233156681060791f},
  {-0.23701065778732299805f, -0.031926624476909637451f, -0.21097345650196075439f, -0.16944588720798492432f, -0.11746002733707427979f, -0.053153056651353836060f, -0.34335806965827941895f}
};
static const float tensor_Gemm_0_bias[10] = 
{-0.68844687938690185547f, 0.59052652120590209961f, -0.025974206626415252686f, 1.5786103010177612305f, 0.32433152198791503906f, 1.2853100299835205078f, -0.22692166268825531006f, -0.32868534326553344727f, -0.30788156390190124512f, 0.055790774524211883545f};
static const float tensor_Gemm_1_weight[3][10] = 
{
  {0.39117076992988586426f, -0.52073997259140014648f, -0.28475266695022583008f, 1.4184408187866210938f, 0.017592662945389747620f, -0.53125423192977905273f, -0.75948047637939453125f, -0.26411843299865722656f, -0.082114040851593017578f, 0.24940845370292663574f},
  {1.2352020740509033203f, -0.47593823075294494629f, -0.30233028531074523926f, -2.1362254619598388672f, -0.11210066825151443481f, -1.6613636016845703125f, 0.98753094673156738281f, 0.17236885428428649902f, 0.11660238355398178101f, -0.15252122282981872559f},
  {-2.1630506515502929688f, 1.3498294353485107422f, 0.14450658857822418213f, 1.0906130075454711914f, 0.10756657272577285767f, 2.0944540500640869141f, -0.14665892720222473145f, 0.17871163785457611084f, -0.18380934000015258789f, 0.24835462868213653564f}
};
static const float tensor_Gemm_1_bias[3] = 
{0.32180559635162353516f, -0.62262642383575439453f, 0.17860612273216247559f};
float tensor_onnx__Gemm_5[1][7];
float tensor_onnx__Gemm_7[1][10];

float tensor_input[1][10];


static inline void node_Flatten_0( const float tensor_onnx__Flatten_0[1][7], float tensor_onnx__Gemm_5[1][7] )
{
	/* Flatten*/
	float *input = (float*)tensor_onnx__Flatten_0;
	float *output = (float*)tensor_onnx__Gemm_5;
	for( uint32_t i=0; i<7; i++ )
		output[i] = input[i];

}

static inline void node_Gemm_1( const float tensor_onnx__Gemm_5[1][7], const float tensor_Gemm_0_weight[10][7], const float tensor_Gemm_0_bias[10], float tensor_input[1][10] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 7;
	const int N = 10;
	float (*A)[7]  = (float(*)[7])tensor_onnx__Gemm_5;
	float (*Y)[10]  = (float(*)[10])tensor_input;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[10]  = (float(*)[10])tensor_Gemm_0_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_0_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node_Relu_2( const float tensor_input[1][10], float tensor_onnx__Gemm_7[1][10] )
{
	/*Relu*/
	float *X = (float*)tensor_input;
	float *Y = (float*)tensor_onnx__Gemm_7;
	for( uint32_t i=0; i<10; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node_Gemm_3( const float tensor_onnx__Gemm_7[1][10], const float tensor_Gemm_1_weight[3][10], const float tensor_Gemm_1_bias[3], float tensor_8[1][3] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 10;
	const int N = 3;
	float (*A)[10]  = (float(*)[10])tensor_onnx__Gemm_7;
	float (*Y)[3]  = (float(*)[3])tensor_8;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[3]  = (float(*)[3])tensor_Gemm_1_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_1_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}


void original(const float tensor_onnx__Flatten_0[1][7], float tensor_8[1][3]) {
	node_Flatten_0( tensor_onnx__Flatten_0, tensor_onnx__Gemm_5);
	node_Gemm_1( tensor_onnx__Gemm_5, tensor_Gemm_0_weight, tensor_Gemm_0_bias, tensor_input);
	node_Relu_2( tensor_input, tensor_onnx__Gemm_7);
	node_Gemm_3( tensor_onnx__Gemm_7, tensor_Gemm_1_weight, tensor_Gemm_1_bias, tensor_8);
}

#endif // ORIGINAL_H

